[TOC]

# Введение в Кафку

Кафка помогает перейти от ETL (extract, transform, load) и батч-обработки к обработке в почти реальном времени

## Что такое Кафка?
Это распределенная платформа потоковой обработки данных
Ее возможности:
* чтение и запись сообщений в очередь
* отказоустойчивое хранение сообщений
* обработка потоков данных по мере получения

Методы доставки:
*  at-least-once -- сообщение отправляется столько раз, сколько нужно, пока не получен ack
*  at-most-once -- сообщение отправляется ровно 1 раз
*  exactly-once -- потребители увидят сообщение ровно 1 раз, даже если производитель отправит его несколько раз

### Кафка для разработчика
Кафка помогает снизить сцепленность кода, если использовать ее с умом
Пример: приложение для создания отпусков отправляет данные в бухгалтерию и в сервис прогнозирования работ. С Кафкой нам не нужно связывать между собой эти 2 потребителя данных

### Кафка с точки зрения менеджера
Позволяет взять большие объемы данных и сделать их доступными в разных бизнес-единицах

## Мифы о Кафке
* Кафка может работать только с Хадупом
    Это не так, причем Кафка может работать даже без HDFS
* Кафка -- это еще один брокер сообщений
    Да, Кафка может использоваться как шина, но есть дополнительные возможности:
      * возможность прочитать сообщение несколько раз (replay). Кафка была создана так, чтобы у очереди могло быть несколько потребителей. Если сообщение прочитано, оно не удаляется из очереди. В частности, приложение может прочесть сколько-то сообщений, а потом взять более раннюю позицию и прочитать их заново. Это может быть полезно в случае программной ошибки.
      * параллельная обработка данных


## Кафка в реальном мире
### Ранние использования
Просто доставить сообщение из пункта А в пункт Б
Кафка предпочла собственный бинарный протокол популярным протоколам типа XMPP, JMS, AMQP
Кафка дает высокую доступность и хранит данные на диске
Кафка может использоваться для агрегации логов
  
### Последующие использования
Сервисы могут общаться асинхронно по Кафке, а не через API
Область применения -- интернет вещей

### Когда Кафка может не подойти
* Когда батч-задачи справляются
* Когда происходит случайный, а не последовательный доступ к данным
* Когда важна упорядоченность сообщений
* Когда размер сообщения велик
    
# Узнаём больше о Кафке

## Что такое брокеры?
Серверная часть Кафки
Isr = in-sync replicas = реплики, у которых нет отставания от лидера
Реплика партиции существует только на одном брокере, не может быть разделена на несколько брокеров

## Обзор Кафки

### Топики
Топики состоят из партиций, партиции состоят из сегментных файлов. У партиции может быть одна или несколько реплик. И только 1 лидер. Пока нет никаких ошибок, чтение и запись происходит только через лидера.

### Использование ZooKeeper
Для того чтобы брокеры пришли к соглашению, какой лидер у какой партиции, нужна координация. За это и отвечает ZooKeeper.
В новых версиях Кафки не используется ZK. 
**Кстати, какая версия у нас используется?**


В реальном кластере используется ансамбль ZK, но для локального запуска достаточно одного процесса ZK.

### Архитектура Кафки
Кафка использует страничный кэш вместо хипа для кэширования. Это позволяет избежать проблем, которые возникают из-за большого хипа (паузы GC).
Также это позволяет быстрее получать недавние сообщения.
### Commit log
Мы всегда только добавляем в конец лога (append-only).
Чтение сообщения не удаляет его из лога.
Можно настраивать хранение сообщений по времени или по размеру. После достижения определенного размера, сообщения могут быть перемещены в постоянное хранилище.

## Программные пакеты

### Kafka Streams
Предоставляет API, похожий на Java Stream API. Используя Kafka Streams, микросервисы могут обмениваться данными, оставаясь независимыми.
### Kafka Connect
Позволяет интегрировать Кафку с другими системами. Например, писать из БД в топик кафки (source). Или писать из топика в облако для длительного хранения (sink).
### AdminClient
Этот API является частью kafka-clients.jar, полезен для администрирования
### ksqlDB
Похоже на SQL, но с возможностью постоянного обновления.

### Клиенты Confluent
Есть клиенты для .NET, C++, Python, Go, etc. Они поддерживают Producer и Consumer API. Не поддерживают Connect и Streams.
#### Java клиент
Передаем Properties для создания KafkaProducer
bootstrap.servers -- это список брокеров, он необязательно должен быть полным
задаем serializers
KafkaProducer потокобезопасен

KafkaConsumer
Задаем deserializers
KafkaConsumer не потокобезопасен

Вопрос: какую группу использует консольный клиент по умолчанию?

## Потоковая обработка и терминология
### Потоковая обработка
Это означает, что данные для обработки постоянно приходят и никогда не закончатся. Код обрабатывает данные постоянно, не ожидая явных запросов или определенного времени.

# Проектирование проекта на Кафке
Приложение получает данные от датчиков на заводе по производству велосипедов

Кафка позволяет достаточно легко восстановиться при программной ошибке, так как сообщения не удаляются. Их можно обработать повторно.

Авторы рекомендуют сначала писать данные в Кафку, а потом в БД.

Почему Кафка хорошо подходит для этого случая?
* Горизонтальное масштабирование
* Постоянный поток данных
* Малый размер 1 сообщения (< 10 kb)

## Проектирование события от датчика

### Требования
* Если потребитель перестал работать, важно, чтобы потом он обработал данные от датчиков
* Хотим знать, работает ли датчик
* Хотим поддерживать историю изменений
* Хотим аудитный лог
* Хотим знать, кто выполнял действия с датчиками

### План
1. Сценарий: пользователь отправляет команду на датчик. Хотим видеть аудитный лог всех этих событий.
Нам важно не допустить потери сообщений и сделать возможным независимое потребление сообщений без доп координации.
2. Сценарий: датчики отправляют свои статусы в Кафку. При отслеживании трендов событий от датчиков можно терять сообщения. 
При этом важна группировка по ключу (свой ключ для каждого сенсора).
3. Сценарий: датчик отправляет сигнал тревоги. Нам нужна группировка по стадии, нужен только последний алерт. 
Лог Кафки неизменяем, но статус можно "обновить" с помощью компактификации лога.

Имеем 3 топика: для аудита, для трендов и для алертов.

## Формат данных
Используем Avro, потому что оно позволяет задавать схему данных и поддерживает изменение этой схемы. 
Avro всегда сериализуется вместе со схемой (если используется Schema Registry, то ее ID)

# Поставщики данных

Пример с отправкой обратной связи от пользователя
Можно при отправке формы генерировать письмо. На письма будет отвечать поддержка, также их могут обрабатывать скрипты.
А можно использовать Кафку, в этом случае можно выбрать любой удобный формат сообщения. Дальше эти данные могут обрабатываться разными потребителями.

## Конфигурация продюсеров
Можно включить идемпотентность (`enable.idempotency`): если отправляем одно и то же сообщение несколько раз, в итоге получим одно сообщение.

`bootstrap.servers`: достаточно указать 1 сервер, продюсер запросит метаданные у брокера и узнает, какой лидер у данной партиции.

`ack`: если значение равно `0`, то лидер не дожидается записи сообщения на репликах. если `all`, то дожидается всех. если `1`, то получаем подтверждение только от лидера.

У сообщения есть timestamp, оно может быть равно времени на клиенте либо времени на брокере (`message.timestamp.type`). 
Если настройка `max.in.flight.requests.per.connection` больше 1, то сообщение, на котором произошел retry, может оказаться в логе после более позднего сообщения, которое отправилось с первой попытки.

Можно написать свой partitioner.

# Потребители

Важно, что потребители запрашивают данные, а не наоборот, производители предлагают их. Потребители контролируют обработку данных, поэтому необязательно, чтобы приложение-потребитель всегда было поднято.

Важные свойства конфига:
* bootstrap.servers
* value.deserializer
* key.deserializer
* group.id
* client.id
* heartbeat.interval.ms 

Как правильно остановить: проставить переменную stopping = true, вызвать wakeup на потребителе. В результате выбросится WakeupException.

## Офсеты
Чтение с самого начала: auto.offset.reset=earliest

Для того чтобы найти сообщение, нужно знать топик, номер партиции и офсет. Чтение происходит из лидера партиции.
(Есть KIP на чтение данных из ближайшей реплики, на случай, если брокеры находятся в разных дата-центрах.)

Но как потребитель поймет, из какой партиции читать, и какой у нее лидер? Для этого у каждой группы потребителей есть брокер — координатор группы.

Нормально иметь больше потребителей, чем партиций. Например, у нас может быть 3 партиции и 4 потребителя. 1 будет запасным, на случай, если упадет какой-то другой.

Обычно офсеты хранятся во внутреннем топике Кафки.

Можем использовать разные group.id, чтобы независимо обрабатывать одни и те же данные двумя разными приложениями.

Для заданных группы, топика и партиции известен офсет.

В каждый момент времени партицию может читать только 1 потребитель. Например, если у нас 3 партиции и 2 потребителя, то 1 потребитель возьмет 2 партиции, а другой -- 1.

partition.assignment.strategy -- бывает range, round-robin, sticky, cooperative sticky (?)

По умолчанию стоит enable.auto.commit=true. Это может вызвать проблемы, если мы обрабатываем данные в другом потоке. В этом случае мы можем автоматически закоммитить, даже если в другом потоке произошла ошибка.

Коммитить можно синхронно и асинхронно. Для асинхронного нужно передать callback, который принимает мапу из TopicPartition в OffsetAndMetadata и исключение (если есть).

Топики бывают compacted (для одного ключа сохраняется последнее значение). Однако бывает так, что для одного ключа может остаться более одной записи. Это происходит, потому что compaction происходит на диске, а в памяти могут быть сообщения с тем же ключом.
Пример такого топика — __consumer_offsets.

## Опции чтения
Чтобы прочитать данные с самого начала топика, можно использовать `auto.offset.reset=earliest` или использовать случайный UUID в качестве ID группы потребителей.

Чтобы прочитать с конца — использовать `auto.offset.reset=latest`.

Настройка `auto.offset.reset` используется, только если офсет не закоммичен для заданной группы.

Для поиска значений офсетов для заданных временных меток можно использовать метод `Consumer#offsetsForTimes()`.

## Велосипедная фабрика
Сообщения аудита: используем явный коммит, чтобы не терять сообщения.

Для обработки трендов алертов можем использовать автокоммит, т.к. потеря сообщений не проблема в этом случае.

Для обработки критических алертов используем assign, т.к. знаем, что критические алерты идут в партицию 0 (такой partitioner у нас). 
Используем асинхронный коммит, чтобы не задерживать обработку алертов.

# Брокеры

## ZooKeeper
Кластер ZooKeeper хранит информацию о топиках и помогает брокерам, координируя назначения и уведомления.

## Управление состоянием
В кластере есть один узел - контроллер, который отвечает за управление кластером, в т.ч. выбирает лидера, обрабатывает падение брокеров.

## Лидеры партиций
Только лидер делает запись данной партиции.
Сообщение коммитится только после того, как оно записалось во все ISR.
Если лидер партиции падает, лидером становится одна из in-sync реплик.
Важно: если оказывается, что последователь слишком медленный, лидер может удалить его из списка ISR.

### Потеря данных
Если задано свойство `unclean.leader.election.enable=true`, контроллер может выбрать лидера, на котором есть не все данные.

## Смотрим в Кафку

### Добавление брокера
Для добавления достаточно просто задать уникальный `broker.id` или поставить в конфиге `broker.id.generation.enable`.
При этом важно понимать, что новый брокер получит только новые топик-партиции.

### Постепенное обновление
Можно обновлять брокеры друг за другом, чтобы избежать downtime.

### Backup
Предпочтительным способом бэкапа является копирование всего кластера. Возможные решения: MirrorMaker, Confluent Replicator, Cluster Linking.

# Топики и партиции
Пример: система для покупки курсов. Есть события поиска, бронирования, оплаты.

Рассмотрим следующие аспекты:
* корректность данных -- данные, которые должны быть упорядочены, должны пойти в одну и ту же партицию. 
логично сделать топик бронирования (ключ = id студента), аналогично топик оплаты. для событий поиска ключ не нужен
* объем сообщений на потребителя -- не стоит делать 1 топик на все сообщения, в этом случае потребителю придется отфильтровывать 99% ненужных сообщений
* насколько много данных нужно обработать -- если надо иметь несколько потребителей одновременно, чтобы справляться с нагрузкой, нужно учесть, что количество потребителей ограничено количеством партиций

Можно сделать топик на поиск и топик на события бронирования и оплаты

Кафка не поддерживает уменьшение числа партиций

## Сколько партиций нам нужно?
Исходим из пропускной способности (Mb / s). Примерная формула для числа партиций:
n = max(t/c, t/p), где t -- желаемая пропускная способность, c -- пропускная способность потребления, p -- пропускная способность записи.

Но нужно учитывать, что чем больше партиций, тем:
* больше открытых файлов
* больше время недоступности. если брокер завершился некорректно (например, через kill), время недоступности будет пропорционально количеству партиций: 
для каждой партиции нужно выбрать нового лидера
* больше задержка
* больше потребление памяти продюсером

### Опции создания топика
Чтобы можно было удалять топик, нужно задать `delete.topic.enable = true`.

## Партиции

### Как хранятся данные партиции
Партиция состоит из нескольких _сегментов_. Сегмент состоит из файлов с расширениями log, index, `timeindex`. Имя файлов сегмента равно первому офсету в этом сегменте.
Размер сегмента равен 1GB по умолчанию.
В файлах с расширением log хранятся данные, а также офсеты и временные метки. 
В файлах .index и `.timeindex`, которые устанавливают соответствие между офсетом (или временем) и физическим положением в файле .log.

## Интеграционное тестирование
Утилита для тестирования EmbeddedKafkaCluster позволяет создать в памяти кластер из нескольких брокеров.
Также можно использовать TestContainers.

## Компактификация топиков
При создании компактного топика задаем cleanup.policy=compact.
Чтобы компактификация работала, нужно, чтобы ключ сообщений не был равен null.
При компактификации берется более позднее сообщение и более поздний офсет.
Старые сегменты компактифицируются, активный сегмент нет.

Пример: храним статус клиента (базовый или золотой). Нас интересует только последнее значение. Ключ равен ID клиента. 
Если хотим удалить клиента, добавляем сообщение со значением null.

# Хранение данных в Кафка

## Насколько долго хранятся данные
По умолчанию данные хранятся неделю, но это можно настроить по времени либо по объему данных (log.retention.bytes, log.retention.ms / minutes / hours).
Можно поставить bytes и ms в -1, тогда все данные будут храниться без ограничений.
Перед удалением из Кафки мы можем сохранить данные в БД или в HDFS.

## Движение данных
Кафка может быть очень полезна для ETL. Она помогает уйти от batch-обработки (ежедневный / ежемесячный процесс). 
С Кафкой данные можно обрабатывать по мере их поступления.

## Инструменты
Flume позволяет перегнать данные в кластер Кафки с помощью конфигурации, без кода. Кафка может использоваться как источник данных, как приемник или как канал.

Debezium отправляет изменения БД как события Кафки. Это CDC -- change data capture.

Secor позволяет отправлять логи Кафки в S3 или Google Cloud Storage. Secor -- это приложение на Java, которое может работать как еще один потребитель данных.

Есть _операционные данные_, они генерируются каждодневными операциями вроде создания заказа. Они актуальны несколько дней, а потом их можно перемещать в _аналитические данные_. 

## Возвращаем данные в Кафку
Ситуация: данные обработали с помощью определенной бизнес-логики и поместили в S3, когда закончился их срок хранения в Кафке. Теперь хотим обработать по новой логике. 
Необязательно забирать данные из S3, можно положить в новый топик с помощью Kafka Connect и обработать этот топик. Таким образом, получаем единый интерфейс для работы с данными -- Кафку.

## Архитектуры с использованием Кафки
### Лямбда
Представление данных реального времени комбинируется с представлением исторических данных.
Есть 3 слоя данных:
* batch - вычисляет представление данных, которые уже есть в системе
* speed - выдает представления данных для недавних данных
* serving - обновляет представления, которые отправляет потребителям
### Каппа
Не делаем слой исторических данных (batch). Просто храним все события, если нужно изменение логики, пересчитываем.

## Несколько кластеров
Можно масштабировать не только за счёт количества брокеров, но и за счет количества кластеров.
## Облачные и контейнерные варианты
Есть Confluent Cloud, Confluent for K8s

# Управление
## Административные клиенты
С помощью API AdminClient можно создавать пользовательские приложения для работы с кластером и его мониторингом.
Можно использовать `kcat` (`kafkacat`) или Confluent REST Proxy API.
## Запуск Kafka в качестве сервиса systemd
`sudo systemctl enable confluent-kafka.service`
## Логирование
Есть 6 типов файлов с логами, например, server.log (kafkaAppender), controller.log (controllerAppender) и т.д.
Для отправки логов в саму Кафку можно использовать `org.apache.kafka.log4jappender.KafkaLog4jAppender`.
В zookeeper.properties можем настроить `autopurge.purgeInterval`, `autopurge.snapRetainCount` (общее количество транзакций), `snapCount` (количество транзакций в файле).
## Брандмауэр
Что происходит, если потребители / производители находятся в другой сети? Или один в контейнере, другой вне контейнера.
Пример: брокер находится в облаке, у него есть внутренний и внешний адрес. При попытке положить сообщение в топик брокер отдает в ответ метаданные со своим внутренним адресом.
В этом случае, помимо `listeners`, в настройках брокера нужно задать `advertised.listeners` -- внешний адрес, чтобы брокер был доступен извне.

Также можно задать для брокера 2 listener'а, один для внутренней коммуникации (с другими брокерами), другой для клиентов.
И задать в конфиге

```inter.broker.listener.name=INTERNAL```
## Метрики
Метрики можно получить также через MBeans, задав опции `JMX_PORT` или `KAFKA_JMX_OPTS`.
Confluent предлагает задать алерты для следующих метрик:
UnderMinIsrPartitionCount, UnderReplicatedPartitions, UnderMinIsr
## Трассировка
Задача: трассировать сообщение.
Делаем интерцептор OnSend, который добавляет заголовок с ID трассировки и пишет его в лог. 
А интерцептор OnConsume его читает и тоже пишет в лог.
Таким образом логика приложения отделяется от логики трассировки.
Реализуем интерфейс `ProducerInterceptor`, добавляем наш интерцептор в `interceptor.classes` в настройках продюсера. 
Интерцепторы могут менять сообщение, поэтому их порядок имеет значение.
Аналогично реализуем интерфейс `ConsumerInterceptor`, добавляем интерцептор в настройки потребителя.

Как вариант, можем использовать кастомную реализацию Consumer и Producer либо использовать готовый TracingConsumer / TracingProducer из библиотеки Brave.
## Инструменты мониторинга
* JMX
* Yammer Metrics

Инструменты, сделанные специально для Кафки:
* CMAK (Kafka Manager) -- инструмент, который позволяет работать с несколькими кластерами
* Cruise Control (REST + UI)
* Confluent Control Center (платный инструмент)

# Защита данных в Кафке

## Основы безопасности
### Шифрование с помощью SSL (TLS)
Добавляем сертификат CA в truststore, свой сертификат в keystore.
Задаем keystore и truststore файлы и пароли к ним в настройках брокера.
В настройках клиента задаем truststore и пароль к нему. Один и тот же конфиг может использоваться и для продюсера, и для потребителя.
### SSL между брокерами
Используем настройку `security.inter.broker.protocol = SSL`

## Kerberos и SASL
Kerberos позволяет осуществлять взаимную аутентификацию, может использоваться как SSO.
SASL можно настраивать через JAAS (Java Authentication and Authorization Service).
Для всех брокеров и клиентов создаем файлы _keytab_ и файлы конфигурации JAAS, добавляем listener SASL_SSL.
## Авторизация в Кафке
### ACL (Access Control Lists)
Можем использовать скрипт `bin/kafka-acls.sh`, задав в конфиге `authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer`. 
Для заданного пользователя и топика можем разрешить чтение и / или запись.
### RBAC (Role-Based Access Control)
В Confluent Platform можно назначить пользователям роли и задавать разрешения на основе ролей.

## ZooKeeper
Задаем `zookeeper.set.acl=true` в конфигах каждого брокера, чтобы нельзя было перезаписать любую информацию в ZK.
## Квоты
Мы можем задать квоты для client.id или пользователя, чтобы побороть DDOS атаки.
Квоты могут быть по объему данных (`producer_byte_rate,consumer_byte_rate`) или по количеству запросов (`request_rate`).
Для задания квот используем скрипт `bin/kafka-configs.sh`.
## Неактивные данные
По умолчанию Кафка не шифрует данные на диске.
### Управляемые варианты
Amazon MSK шифрует данные, а также инкапсулирует другие опции ИБ. 
Confluent Cloud тоже.

# Schema registry

## Предлагаемая модель зрелости использования Кафки

### Уровень 0
Используем Кафку как ESB (сервисную шину предприятия) или pub / sub систему. Кафка может быть просто заменой RabbitMQ и т.д.
### Уровень 1
Бо́льшая часть данных попадает в Кафку. События собираются через ETL и CDC (change data capture).
Имеем поток данных в реальном времени, который позволяет быстро скормить данные аналитической системе.

Пример: есть БД вендора с информацией о клиента. Мы не хотим, чтобы маркетологи перегружали БД сложными запросами.
Можем использовать Kafka Connect для захвата изменений данных из реляционной БД.
### Уровень 2
Добавляем реестр схем. Это позволяет нам поддерживать обратную совместимость.

### Уровень 3
Все в приложении является потоком событий, который никогда не заканчивается.

## Confluent Schema Registry
Schema Registry позволяет хранить именованные схемы и версионировать их.

Версии могут понадобиться, потому что иногда бывает нужно обработать старые данные заново.

SR использует Кафку для хранения схем, топик `_schemas` по умолчанию. В production имеет смысл разворачивать SR отдельно от брокеров и, возможно, на нескольких машинах. Одна из машин будет главной.

Для сериализации может использоваться JSON, Protobuf, Avro. В примере ниже используется Avro.

### SR REST API
Есть следующие ресурсы:
* schemas
* subjects
* compatibility
* config
По умолчанию для каждого топика создается subject с именем <topic name> + '-value' (для сериализации значения) или '-key' (для сериализации ключа).
Разделение subject на ключи и значения нужно, чтобы типы ключей и значений могли эволюционировать по отдельности.
### Клиентская библиотека SR
При записи данных используем KafkaAvroSerializer, при чтении KafkaAvroDeserializer (из библиотеки Confluent kafka-avro-serializer).

## Правила совместимости
Возможные значения типа совместимости: 
* BACKWARD (по умолчанию)
* BACKWARD_TRANSITIVE
* FORWARD
* FORWARD_TRANSITIVE
* FULL
* FULL_TRANSITIVE
* NONE
Transitive означает, что мы сравниваем новую версию схемы со всем предыдущими, а не только с последней.

### Backward
С помощью новой схемы можно прочитать данные, записанные со старой схемой.
Можем удалять поля или добавлять опциональные поля.
Сначала обновляем консьюмеров, чтобы они могли прочитать новые данные, а потом уже продюсеров.

### Forward
С помощью старой схемы можно прочитать данные, записанные с новой схемой.
Можем добавлять поля и удалять опциональные поля.
Сначала обновляем продюсеров и убеждаемся, что данные со старой схемой недоступны консьюмерам, а потом обновляем консьюмеров.

Больше подробностей [тут](https://docs.confluent.io/platform/current/schema-registry/fundamentals/schema-evolution.html)

Совместимость можно проверять через REST `POST /compatibility/subjects/kinaction_schematest-value/versions/latest` или Maven goal test-compatibility.

## Альтернатива SR
Можно создавать новый топик, если происходит ломающее изменение, и обновлять продюсеров и консьюмеров.
Если надо заново обработать старые данные, можно перегнать старые данные в новый формат (и новый топик) с помощью Kafka Streams.

# Потоковая обработка с Kafka Streams
## Kafka Streams
Что такое потоковая обработка (stream processing / streaming)? Это процесс или приложение, который обрабатывает данные по мере их поступления.
Мы не запускаем процесс по расписанию и не делаем запросов к БД.

Kafka Streams - это библиотека, для ее использования не нужно создавать дополнительный кластер.

При использовании Kafka Streams не нужно ждать, пока накопится батч сообщений, сообщение обработается, как только оно поступило. 

## KStreams API
Функциональный DSL. Главная концепция - ориентированный ацикличный граф.
У графа есть вершина-источник (source processor).

В нашем случае source - это приложение, которое получает транзакции из внешней системы.
Стоки - это топики transaction-success, transaction-failure.

Описываем топологию нашего приложения с помощью DSL. Нет необходимости явно прописывать продюсеры или консьюмеры.

### KTable
Позволяет сделать компактификацию по ключу по каждой партиции. Чем-то похоже на обновление полей в таблице БД.

### GlobalKTable
Позволяет сделать компактификацию по ключу по каждой партиции по всем партициям.
Например, можем сделать `GlobalKTable<String, Customer>`, чтобы хранить информацию о клиенте, в том числе e-mail.
Можно делать join между KStream и GlobalKTable.

?? А что за state store и при чем тут RocksDb?

### Processor API
Работаем с объектом `Topology`, добавляем истоки, стоки и процессоры с помощью `addSource()`, `addSink()` и `addProcessor()`.

Получается больше кода, но и больше контроля. Например, можно задать `AutoOffsetReset` (earliest / latest).

### Развертывание Kafka Streams
Имеет смысл развертывать столько экземпляров KS, сколько партиций (плюс, возможно, резервные на случай отказов).

KS поддерживает at-least-once и exactly-once семантику.

## ksqlDB
БД для потоковой обработки событий.
Основные понятия: стрим (поток) и таблица, которые используют на данные из топиков.
### Запросы
Запросы в ksql бывают persistent, push и pull.
Persistent - запросы, которые создают стримы и таблицы, производные от существующих.
Pull - обычные синхронные запросы, на которые сразу получаем ответ.
Push - асинхронные запросы, подразумевают добавление данных в реальном времени.

Пример push-запроса:
```sql
CREATE
STREAM TRANSACTION_STATEMENT AS
SELECT *
FROM TRANSACTION_SUCCESS
         LEFT JOIN ACCOUNT ON TRANSACTION_SUCCESS.numkey = ACCOUNT.numkey EMIT CHANGES;
```
Соединяем стрим с таблицей, получаем стрим.
### Архитектура ksqlDB
В отличие от Kafka Streams, нам нужен дополнительный компонент, ksqlDB server.
Он предоставляет REST API.

Возможен запуск в headless режиме, при котором запрещены интерактивные команды. Для этого надо передать SQL файл на запуске.